# Day-04 : RLHF and Optimization

**Reinforcement Learning from Human Feedback (RLHF) and Optimization Methods**

Understanding RLHF: Explore Reinforcement Learning from Human Feedback (RLHF) and its applications in improving the quality of language models.

DPO and PPO Over RLHF: Learn how Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) methods improve over RLHF in certain tasks.

Hands-on: RLHF Implementation: Implement RLHF on custom data to see how feedback-driven reinforcement can be used to optimize LLMs.
